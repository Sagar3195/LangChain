{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7544a18",
   "metadata": {},
   "source": [
    "### Tools In Langchain\n",
    " 1. How To Create Tools\n",
    " 2. How to use built-in tools and toolkits\n",
    " 3. How to use chat models to call tools\n",
    " 4. How to pass tool outputs to chat models\n",
    "\n",
    " - This @tool decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ea2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tool decrator \n",
    "\n",
    "from langchain_core.tools import tool \n",
    "\n",
    "@tool\n",
    "def division(a:int, b: int) -> int:\n",
    "    \"\"\"Divivde two numbers\"\"\"\n",
    "    return a/b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division\n",
      "Divivde two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(division.name)\n",
    "print(division.description)\n",
    "print(division.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#async implmentation \n",
    "@tool\n",
    "\n",
    "async def multiply(a:int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List \n",
    "\n",
    "@tool \n",
    "def multiply_by_max(\n",
    "    a: Annotated[int, \"A integer value\"],\n",
    "    b: Annotated[List[int], \"List of integers over which to take maximum\"]\n",
    ") -> int:\n",
    "    \"\"\" Multiply a by maximum of b \"\"\"\n",
    "    \n",
    "    return a * max(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c41605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'description': 'A integer value', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'List of integers over which to take maximum', 'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply_by_max.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97ede9dc",
   "metadata": {},
   "source": [
    "#### Structured Tool\n",
    " - The StructuredTool.from_function class method provides a bit more configurability than the @tool decorator, without requiring much additional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def multiply(a:int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b \n",
    "\n",
    "\n",
    "async def amultiply(a:int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4d370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "calculator = StructuredTool.from_function(func= multiply, coroutine= amultiply)\n",
    "\n",
    "print(calculator.invoke({\"a\": 2,\"b\": 5}))\n",
    "print(await calculator.ainvoke({\"a\": 5, \"b\": 6}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219547cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a35b9fc",
   "metadata": {},
   "source": [
    "### Inbuilt Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia \n",
    "from langchain_community.tools import WikipediaQueryRun \n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results = 3, doc_content_chars_max = 300)\n",
    "tool = WikipediaQueryRun(api_wrapper= api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb12017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis a\n"
     ]
    }
   ],
   "source": [
    "print(tool.invoke({\"query\": \"Langchain\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c1ae5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TavilySearch \n",
    "from dotenv import load_dotenv \n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b782a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch \n",
    "\n",
    "tool = TavilySearch(max_results= 5, \n",
    "                    topics= 'general',\n",
    "                    # include_answer=False,\n",
    "                    # include_raw_content=False,\n",
    "                    # include_images=False,\n",
    "                    # include_image_descriptions=False,\n",
    "                    # search_depth=\"basic\",\n",
    "                    # time_range=\"day\",\n",
    "                    # include_domains=None,\n",
    "                    # exclude_domains=None\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84211dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is recent AI news?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.artificialintelligence-news.com/',\n",
       "   'title': 'AI News | Latest AI News, Analysis & Events',\n",
       "   'content': 'Gen AI makes no financial difference in 95% of cases · Google Cloud unveils AI ally for security teams · New Nvidia Blackwell chip for China may outpace H20',\n",
       "   'score': 0.98504,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.wsj.com/tech/ai?gaa_at=eafs&gaa_n=ASWzDAj9zRjmFQPDkc1CMXZBH7CgxYWMAiGeEMJoILFbbrY-9VmI5R7He1DD&gaa_ts=68a6e46c&gaa_sig=S1D8BnNR5zvzvGs0mlMuKvsop307yrhx9QBHMzdGpwDL42CEdAhFyomGGteuKrVZGnAVuVjpAExagztfk6RTcA%3D%3D',\n",
       "   'title': 'Artificial Intelligence - Latest AI News and Analysis',\n",
       "   'content': 'Artificial Intelligence · Meta Freezes AI Hiring After Blockbuster Spending Spree · GM Raided Silicon Valley to Build Its New AI Team. · Google Is Beating Apple',\n",
       "   'score': 0.9834,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://techcrunch.com/category/artificial-intelligence/',\n",
       "   'title': 'AI News & Artificial Intelligence',\n",
       "   'content': 'News coverage on artificial intelligence and machine learning tech, the companies building them, and the ethical issues AI raises today.',\n",
       "   'score': 0.97817,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://news.mit.edu/topic/artificial-intelligence2',\n",
       "   'title': 'Artificial intelligence | MIT News | Massachusetts Institute of ...',\n",
       "   'content': 'New algorithms enable efficient machine learning with symmetric data. This new approach could lead to enhanced AI models for drug and materials discovery.',\n",
       "   'score': 0.97437,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.nbcnews.com/artificial-intelligence',\n",
       "   'title': 'Artificial intelligence',\n",
       "   'content': \"The latest news and top stories on artificial intelligence, including AI chatbots like Microsoft's ChatGPT, Apple's AI Chatbot and Google's Bard.\",\n",
       "   'score': 0.97111,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.86,\n",
       " 'request_id': 'c88a8bb8-25c5-4141-ae5f-38307c6f4c53'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"What is recent AI news?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3a916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf20024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiv \n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "tool = ArxivAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5913c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2023-08-02\\nTitle: Attention Is All You Need\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\nSummary: The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"1706.03762\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e0944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97636830",
   "metadata": {},
   "source": [
    "### Call Tool With LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a79c4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia \n",
    "from langchain_community.tools import WikipediaQueryRun \n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results = 3, doc_content_chars_max = 300)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769ab509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch \n",
    "\n",
    "tavily_tool = TavilySearch(max_results= 5, \n",
    "                    topics= 'general',\n",
    "                    # include_answer=False,\n",
    "                    # include_raw_content=False,\n",
    "                    # include_images=False,\n",
    "                    # include_image_descriptions=False,\n",
    "                    # search_depth=\"basic\",\n",
    "                    # time_range=\"day\",\n",
    "                    # include_domains=None,\n",
    "                    # exclude_domains=None\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b85b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Tool \n",
    "from langchain_core.tools import tool \n",
    "\n",
    "@tool \n",
    "def add(a : int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b \n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> tool:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f2f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia tavily_search\n"
     ]
    }
   ],
   "source": [
    "print(wiki_tool.name, tavily_tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7befcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of tools \n",
    "tools = [wiki_tool, tavily_tool, add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7dc4e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\PythonCSV\\\\CSV_file\\\\LangChainBasics\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=300)),\n",
       " TavilySearch(max_results=5, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'), api_base_url=None)),\n",
       " StructuredTool(name='add', description='Add two numbers', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x000002470F33B380>),\n",
       " StructuredTool(name='multiply', description='Multiply two numbers', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x000002470DDFBE20>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5213e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002470F5B2F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002470F5B3B60>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create LLM Model \n",
    "from langchain_groq import ChatGroq \n",
    "llm = ChatGroq(model = \"deepseek-r1-distill-llama-70b\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8291d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binding LLM with Tool \n",
    "llm_with_tool = llm.bind_tools(tools) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47e3158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002470F5B2F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002470F5B3B60>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'tavily_search', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'Search query to look up', 'type': 'string'}, 'include_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to restrict search results to.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests information from specific websites (e.g., \"Find climate data from nasa.gov\")\\n        2. The user mentions an organization or company without specifying the domain (e.g., \"Find information about iPhones from Apple\")\\n\\n        In both cases, you should determine the appropriate domains (e.g., [\"nasa.gov\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will ONLY come from the specified domains - no other sources will be included.\\n        Default is None (no domain restriction).\\n        '}, 'exclude_domains': {'anyOf': [{'items': {'type': 'string'}, 'type': 'array'}, {'type': 'null'}], 'default': [], 'description': 'A list of domains to exclude from search results.\\n\\n        Use this parameter when:\\n        1. The user explicitly requests to avoid certain websites (e.g., \"Find information about climate change but not from twitter.com\")\\n        2. The user mentions not wanting results from specific organizations without naming the domain (e.g., \"Find phone reviews but nothing from Apple\")\\n\\n        In both cases, you should determine the appropriate domains to exclude (e.g., [\"twitter.com\"] or [\"apple.com\"]) and set this parameter.\\n\\n        Results will filter out all content from the specified domains.\\n        Default is None (no domain exclusion).\\n        '}, 'search_depth': {'anyOf': [{'enum': ['basic', 'advanced'], 'type': 'string'}, {'type': 'null'}], 'default': 'basic', 'description': 'Controls search thoroughness and result comprehensiveness.\\n    \\n        Use \"basic\" for simple queries requiring quick, straightforward answers.\\n        \\n        Use \"advanced\" (default) for complex queries, specialized topics, \\n        rare information, or when in-depth analysis is needed.\\n        '}, 'include_images': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': 'Determines if the search returns relevant images along with text results.\\n   \\n        Set to True when the user explicitly requests visuals or when images would \\n        significantly enhance understanding (e.g., \"Show me what black holes look like,\" \\n        \"Find pictures of Renaissance art\").\\n        \\n        Leave as False (default) for most informational queries where text is sufficient.\\n        '}, 'time_range': {'anyOf': [{'enum': ['day', 'week', 'month', 'year'], 'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Limits results to content published within a specific timeframe.\\n        \\n        ONLY set this when the user explicitly mentions a time period \\n        (e.g., \"latest AI news,\" \"articles from last week\").\\n        \\n        For less popular or niche topics, use broader time ranges \\n        (\"month\" or \"year\") to ensure sufficient relevant results.\\n   \\n        Options: \"day\" (24h), \"week\" (7d), \"month\" (30d), \"year\" (365d).\\n        \\n        Default is None.\\n        '}, 'topic': {'anyOf': [{'enum': ['general', 'news', 'finance'], 'type': 'string'}, {'type': 'null'}], 'default': 'general', 'description': 'Specifies search category for optimized results.\\n   \\n        Use \"general\" (default) for most queries, INCLUDING those with terms like \\n        \"latest,\" \"newest,\" or \"recent\" when referring to general information.\\n\\n        Use \"finance\" for markets, investments, economic data, or financial news.\\n\\n        Use \"news\" ONLY for politics, sports, or major current events covered by \\n        mainstream media - NOT simply because a query asks for \"new\" information.\\n        '}, 'include_favicon': {'anyOf': [{'type': 'boolean'}, {'type': 'null'}], 'default': False, 'description': \"Determines whether to include favicon URLs for each search result.\\n        \\n        When enabled, each search result will include the website's favicon URL,\\n        which can be useful for:\\n        - Building rich UI interfaces with visual website indicators\\n        - Providing visual cues about the source's credibility or brand\\n        - Creating bookmark-like displays with recognizable site icons\\n        \\n        Set to True when creating user interfaces that benefit from visual branding\\n        or when favicon information enhances the user experience.\\n        \\n        Default is False to minimize response size and API usage.\\n        \"}, 'start_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or after this date.\\n        \\n        Use this parameter when you need to:\\n        - Find recent developments or updates on a topic\\n        - Exclude outdated information from search results\\n        - Focus on content within a specific timeframe\\n        - Combine with end_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-01-15\" for January 15, 2024).\\n        \\n        Examples:\\n        - \"2024-01-01\" - Results from January 1, 2024 onwards\\n        - \"2023-12-25\" - Results from December 25, 2023 onwards\\n        \\n        When combined with end_date, creates a precise date range filter.\\n        \\n        Default is None (no start date restriction).\\n        '}, 'end_date': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Filters search results to include only content published on or before this date.\\n        \\n        Use this parameter when you need to:\\n        - Exclude content published after a certain date\\n        - Study historical information or past events\\n        - Research how topics were covered during specific time periods\\n        - Combine with start_date to create a custom date range\\n        \\n        Format must be YYYY-MM-DD (e.g., \"2024-03-31\" for March 31, 2024).\\n        \\n        Examples:\\n        - \"2024-03-31\" - Results up to and including March 31, 2024\\n        - \"2023-12-31\" - Results up to and including December 31, 2023\\n        \\n        When combined with start_date, creates a precise date range filter.\\n        For example: start_date=\"2024-01-01\", end_date=\"2024-03-31\" \\n        returns results from Q1 2024 only.\\n        \\n        Default is None (no end date restriction).\\n        '}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'add', 'description': 'Add two numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'multiply', 'description': 'Multiply two numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48a8f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 2, 'b': 5}, 'id': 'fewhat3by', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "question = \"What is 2 * 5?\"\n",
    "messages = [HumanMessage(question)]\n",
    "response = llm_with_tool.invoke(question)\n",
    "print(response.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39476c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0899cfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 2 * 5?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='10', name='multiply', tool_call_id='fewhat3by'),\n",
       " ToolMessage(content='10', name='multiply', tool_call_id='fewhat3by')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply, \"wikipedia\": wiki_tool, \"tavily_search\": tavily_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb1e4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The product of multiplying 2 and 5 is 10.\\n\\n', additional_kwargs={'reasoning_content': \"Alright, so I need to figure out what 2 multiplied by 5 is. Hmm, I remember from basic math that multiplication is like adding a number to itself a certain number of times. So, 2 times 5 should mean adding 2 five times. Let me see: 2 + 2 is 4, plus another 2 is 6, then 8, and finally 10. So, 2 multiplied by 5 equals 10. I think that's right because when I add 2 five times, I get 10. Yeah, that makes sense. I don't see any mistakes in that calculation.\\n\", 'tool_calls': [{'id': '1tg2bpg14', 'function': {'arguments': '{\"a\":2,\"b\":5}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 1816, 'total_tokens': 2016, 'completion_time': 0.736724837, 'prompt_time': 0.172824869, 'queue_time': 0.046017771, 'total_time': 0.909549706}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--319b5331-e3e8-430d-9b85-aa06c3b0c564-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 5}, 'id': '1tg2bpg14', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1816, 'output_tokens': 200, 'total_tokens': 2016})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b001e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee570542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, so the user is asking two things: \"What is langchain?\" and \"What is 5 * 15?\" I need to address both parts. \\n\\nFirst, for the \"What is langchain?\" part, I should probably use a tool that can provide information. The options are the Wikipedia tool or the Tavily search tool. Since LangChain is a known library in the AI and machine learning space, I think the Tavily search would be more appropriate because it\\'s optimized for current and comprehensive results. I\\'ll set the search query to \"langchain\" and maybe include some parameters to get the most relevant information.\\n\\nSecond, for the multiplication part, \"5 * 15,\" that\\'s a straightforward math problem. I can use the multiply function provided. It requires two integers, a and b, so I\\'ll set a to 5 and b to 15.\\n\\nI should make sure to present both answers clearly, one after the other. First, the information about LangChain, then the result of the multiplication. I\\'ll format each part as a separate tool call in the response.\\n', 'tool_calls': [{'id': 'amc9tf7ka', 'function': {'arguments': '{\"end_date\":null,\"exclude_domains\":[],\"include_domains\":[],\"include_favicon\":false,\"include_images\":false,\"query\":\"langchain\",\"search_depth\":\"advanced\",\"start_date\":null,\"time_range\":null,\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 337, 'prompt_tokens': 1798, 'total_tokens': 2135, 'completion_time': 1.207838905, 'prompt_time': 0.170960176, 'queue_time': 0.046174684, 'total_time': 1.378799081}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2d57a57f-a16a-4006-b93a-6b200cddc168-0', tool_calls=[{'name': 'tavily_search', 'args': {'end_date': None, 'exclude_domains': [], 'include_domains': [], 'include_favicon': False, 'include_images': False, 'query': 'langchain', 'search_depth': 'advanced', 'start_date': None, 'time_range': None, 'topic': 'general'}, 'id': 'amc9tf7ka', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1798, 'output_tokens': 337, 'total_tokens': 2135})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is langchain and what is 5 * 15?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tool.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75aecf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search',\n",
       "  'args': {'end_date': None,\n",
       "   'exclude_domains': [],\n",
       "   'include_domains': [],\n",
       "   'include_favicon': False,\n",
       "   'include_images': False,\n",
       "   'query': 'langchain',\n",
       "   'search_depth': 'advanced',\n",
       "   'start_date': None,\n",
       "   'time_range': None,\n",
       "   'topic': 'general'},\n",
       "  'id': 'amc9tf7ka',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659e530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "868c511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is langchain and what is 5 * 15?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='{\"query\": \"langchain\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://medium.com/data-engineering-with-dremio/crash-course-on-developing-ai-applications-with-langchain-b6b25d0e6ea5\", \"title\": \"Crash Course on Developing AI Applications with LangChain\", \"content\": \"By providing a modular approach, LangChain allows you to compose **prompt templates, chains, memory, and agents** to build flexible and scalable solutions. Instead of manually handling prompts, API calls, and responses, LangChain provides a structured way to **chain together different components** such as prompts, memory, and external tools. Now that we have LangChain installed, let’s explore how to use it for interacting with LLMs, structuring prompts, and building simple AI workflows. Now that you understand the basics of LangChain — connecting to LLMs, structuring prompts, using chains, memory, and agents — let’s discuss some best practices for building efficient and scalable applications. By combining **prompt engineering, chains, memory, and agents**, LangChain simplifies the development process, making it easier to create **chatbots, AI assistants, and retrieval-augmented generation (RAG) applications**.\", \"score\": 0.98574, \"raw_content\": null}, {\"url\": \"https://aws.amazon.com/what-is/langchain/\", \"title\": \"What is LangChain? - AWS\", \"content\": \"LangChain is an open source framework for building applications based on large language models (LLMs). For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain provides AI developers with tools to connect language models with external data sources. With LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome. Developers then use the chain building blocks or LangChain Expression Language (LCEL) to compose chains with simple programming commands. Developers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs.\", \"score\": 0.98534, \"raw_content\": null}, {\"url\": \"https://www.techtarget.com/searchenterpriseai/definition/LangChain\", \"title\": \"What Is LangChain and How to Use It: A Guide - TechTarget\", \"content\": \"LangChain is an open source framework that enables software developers working with artificial intelligence (AI) and its machine learning subset to combine large language models with other external components to develop LLM-powered applications. LangChain aims to link powerful LLMs, such as OpenAI\\'s GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications. * **Prompt templates.** LangChain includes prompt template modules that enable developers to create structured prompts for LLMs. These templates can incorporate examples and specify output formats, facilitating smoother interactions and more accurate responses from the models. The LangChain framework helps developers create LLM-powered applications by offering tools that build intricate workflows and integrate different components. * ##### How to use LangChain for LLM application development\", \"score\": 0.98378, \"raw_content\": null}, {\"url\": \"https://www.langchain.com/\", \"title\": \"LangChain\", \"content\": \"# The platform for reliable agents. Tools for every step of the agent development lifecycle -- built to unlock powerful AI\\xa0in production. ###### Build agents with LangGraph Evaluate and observe agent performance at scale.###### DEPLOYMENT: STACK 1:\\xa0LangGraph +\\xa0LangChain +\\xa0LangSmith +\\xa0LangGraph\\xa0Platform ### A full product suite for reliable agents and LLM apps Trace using the TypeScript or Python SDK\\xa0to gain visibility into your agent interactions -- whether you use LangChain\\'s frameworks or not. STACK 3:\\xa0Any agent framework +\\xa0LangGraph Platform LangGraph Platform works with *any* agent framework, enabling stateful UXs like human-in-the-loop and streaming-native deployments. Get started with tools from the LangChain product suite for every step of the agent development lifecycle.\", \"score\": 0.98108, \"raw_content\": null}, {\"url\": \"https://python.langchain.com/docs/introduction/\", \"title\": \"Introduction | 🦜 LangChain\", \"content\": \"*   Build a simple LLM application with chat models and prompt templates *   How to migrate from legacy LangChain agents to LangGraph *   How to use chat models to call tools *   How to pass tool outputs to chat models *   **`langchain`**: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture. However, these guides will help you quickly accomplish common tasks using chat models, vector stores, and other common LangChain components. LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you\\'re looking to get up and running quickly with chat models, vector stores, or other LangChain components from a specific provider, check out our growing list of integrations.\", \"score\": 0.97942, \"raw_content\": null}], \"response_time\": 1.12, \"request_id\": \"62937d7a-285c-43d5-b7b6-a1a82f48bca4\"}', name='tavily_search', tool_call_id='amc9tf7ka')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply,\"wikipedia\":wiki_tool,\"tavily_search\":tavily_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d3605e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1=llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31101196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain is a framework designed to build applications using large language models (LLMs) by integrating them with external data sources and creating structured workflows. It provides tools for developers to create prompt templates, agents, and cognitive architectures, enabling the development of LLM-powered applications efficiently.\\n\\nAs for the multiplication, 5 multiplied by 15 equals 75.\\n\\nStep-by-step explanation:\\n1. **Understanding LangChain**: LangChain is a framework that helps developers build applications by connecting LLMs with external components, allowing for structured workflows and integrations.\\n2. **Multiplication Calculation**: \\n   - 5 * 15 can be broken down into (5 * 10) + (5 * 5) = 50 + 25 = 75.\\n   - Alternatively, adding 15 five times: 15 + 15 + 15 + 15 + 15 = 75.\\n\\nFinal Answer:\\n5 * 15 = 75.', additional_kwargs={'reasoning_content': \"Okay, so I need to figure out what LangChain is and calculate 5 multiplied by 15. Let's start with LangChain. From the search results, it seems LangChain is a framework for building applications using large language models (LLMs). It helps developers connect LLMs with external data sources and build applications by chaining together different components like prompts, memory, and agents. It provides tools and libraries to create structured prompts and integrate various components for more complex applications.\\n\\nNow, for the multiplication part, 5 times 15. I know that 5 multiplied by 10 is 50, and 5 multiplied by 5 is 25, so adding those together gives 75. Alternatively, I can think of it as 15 added five times: 15 + 15 is 30, plus 15 is 45, plus 15 is 60, and one more 15 makes 75. So, 5 * 15 equals 75.\\n\\nI should make sure I didn't make any calculation errors. Let me double-check: 5 times 15 is indeed 75. Yep, that's correct.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 427, 'prompt_tokens': 2958, 'total_tokens': 3385, 'completion_time': 1.5310201540000001, 'prompt_time': 0.274606075, 'queue_time': 0.046512565, 'total_time': 1.805626229}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--67a1c7e0-9a29-4ff3-aa71-d7ed4da712b4-0', usage_metadata={'input_tokens': 2958, 'output_tokens': 427, 'total_tokens': 3385})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d02f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65ec3ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Let's tackle both of your questions:\\n\\n1. **Math Calculation:**\\n   - **5 * 15 = 75**\\n\\n2. **Information about Agentic AI:**\\n   Let me search for information about Agentic AI.\\n\\n\", additional_kwargs={'reasoning_content': 'Okay, so the user is asking two things here: first, they want to know what 5 multiplied by 15 is, and second, they\\'re asking about Agentic AI. Let me break this down step by step.\\n\\nStarting with the math part, multiplying 5 by 15. I remember that 5 times 10 is 50, and then 5 more 5s would be 25, so adding those together gives 75. That seems straightforward.\\n\\nNow, for the Agentic AI part. I\\'m not exactly sure what that is. I think it might be related to AI, but I\\'m not certain. Maybe it\\'s a company or a specific AI model. I should look it up to get accurate information. I\\'ll use the Tavily search tool for this since it\\'s good for finding current and comprehensive information.\\n\\nI need to set up the tool call properly. The function name is \"tavily_search\" and the query should be \"Agentic AI\". I don\\'t need any specific dates or domains, so I\\'ll leave those parameters empty or set to their default values. Also, including images or favicons isn\\'t necessary here since the user just wants information.\\n\\nAfter getting the search results, I should summarize what Agentic AI is, making sure to provide a clear and concise answer. I\\'ll present both the math answer and the information about Agentic AI in a friendly and helpful manner.\\n', 'tool_calls': [{'id': '8fra71ckd', 'function': {'arguments': '{\"end_date\":null,\"exclude_domains\":[],\"include_domains\":[],\"include_favicon\":false,\"include_images\":false,\"query\":\"Agentic AI\",\"search_depth\":\"advanced\",\"start_date\":null,\"time_range\":null,\"topic\":\"general\"}', 'name': 'tavily_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 426, 'prompt_tokens': 1800, 'total_tokens': 2226, 'completion_time': 1.628233463, 'prompt_time': 0.17147606, 'queue_time': 0.04588199, 'total_time': 1.799709523}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--163cbc77-5079-499a-b16c-0711d969b93a-0', tool_calls=[{'name': 'tavily_search', 'args': {'end_date': None, 'exclude_domains': [], 'include_domains': [], 'include_favicon': False, 'include_images': False, 'query': 'Agentic AI', 'search_depth': 'advanced', 'start_date': None, 'time_range': None, 'topic': 'general'}, 'id': '8fra71ckd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1800, 'output_tokens': 426, 'total_tokens': 2226})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is 5 * 15 and also What is Agentic AI?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tool.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cad35079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search',\n",
       "  'args': {'end_date': None,\n",
       "   'exclude_domains': [],\n",
       "   'include_domains': [],\n",
       "   'include_favicon': False,\n",
       "   'include_images': False,\n",
       "   'query': 'Agentic AI',\n",
       "   'search_depth': 'advanced',\n",
       "   'start_date': None,\n",
       "   'time_range': None,\n",
       "   'topic': 'general'},\n",
       "  'id': '8fra71ckd',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2400400f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 5 * 15 and also What is Agentic AI?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='{\"query\": \"Agentic AI\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://en.wikipedia.org/wiki/Agentic_AI\", \"title\": \"Agentic AI - Wikipedia\", \"content\": \"Agentic AI is a class of artificial intelligence that focuses on autonomous systems that can make decisions and perform tasks without human intervention. The independent systems automatically respond to conditions, to produce process results. The field is closely linked to agentic automation, also known as agent-based process management systems, when applied to process automation. Applications include software development, customer support, cybersecurity and business intelligence. [...] The core concept of agentic AI is the use of _AI agents_ to perform automated tasks but without human intervention.( While robotic process automation (RPA) and AI agents can be programmed to automate specific tasks or support rule-based decisions, the rules are usually fixed.( Agentic AI operates independently, making decisions through continuous learning and analysis of external data and complex data sets.( Functioning agents can require various AI techniques, such as natural language [...] In the last decade, significant advances in AI have spurred the development of agentic AI. Breakthroughs in deep learning, reinforcement learning, and neural networks allowed AI systems to learn on their own and make decision with minimal human guidance._[citation needed_] Consilience of agentic AI across autonomous transportation, industrial automation, and tailored healthcare has also supported its viability. Self-driving cars use agentic AI to handle complex road scenarios, though with some\", \"score\": 0.9645393, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/agentic-ai\", \"title\": \"What Is Agentic AI? | IBM\", \"content\": \"## Authors\\\\n\\\\nCole Stryker\\\\n\\\\nEditorial Lead, AI Models\\\\n\\\\n## What is agentic AI?\\\\n\\\\nAgentic AI is an artificial intelligence system that can accomplish a specific goal with limited supervision. It consists of AI agents—machine learning models that mimic human decision-making to solve problems in real time. In a multiagent system, each agent performs a specific subtask required to reach the goal and their efforts are coordinated through AI orchestration. [...] Unlike traditional AI models, which operate within predefined constraints and require human intervention, agentic AI exhibits autonomy, goal-driven behavior and adaptability. The term “agentic” refers to these models’ agency, or, their capacity to act independently and purposefully. [...] Agentic AI builds on generative AI (gen AI) techniques by using large language models (LLMs) to function in dynamic environments. While generative models focus on creating content based on learned patterns, agentic AI extends this capability by applying generative outputs toward specific goals. A generative AI model like OpenAI’s ChatGPT might produce text, images or code, but an agentic AI system can use that generated content to complete complex tasks autonomously by calling external tools.\", \"score\": 0.9531401, \"raw_content\": null}, {\"url\": \"https://www.moveworks.com/us/en/resources/blog/agentic-ai-examples-use-cases\", \"title\": \"6 Agentic AI Examples and Use Cases Transforming Businesses\", \"content\": \"Agentic AI refers to artificial intelligence systems that are capable of independently pursuing goals and managing complex workflows with minimal human oversight. These AI systems are highly adaptive and can understand user needs based on business context, then autonomously formulate a plan and take specific real-world actions to accomplish those goals. [...] In addition to IT support, agentic AI is playing an increasingly critical role in enhancing security measures within organizations. Agentic AI enables security teams to help predict and proactively address quickly evolving and looming security threats.\\\\n\\\\nAccording to the NCSC, “AI can make cyber attacks… more impactful because threat actors will be able to analyse exfiltrated data faster and more effectively, and use it to train AI models.” [...] Agentic AI can go well beyond standard AI chatbot or smart assistant abilities like answering questions, giving recommendations, recognizing images, or helping to manage schedules.\\\\n\\\\nThat’s because agentic AI agents are engineered to autonomously execute complex, multi-step processes with little to no supervision. Unlike many chatbots or generative AI tools, which respond to user inputs based on predefined rules, AI agents are able to:\\\\n\\\\n   Reason\\\\n   Plan\\\\n   Execute\\\\n   Adapt\", \"score\": 0.95132804, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/agentic-ai-vs-generative-ai\", \"title\": \"Agentic AI vs. Generative AI - IBM\", \"content\": \"Agentic AI describes AI systems that are designed to autonomously make decisions and act, with the ability to pursue complex goals with limited supervision. It brings together the flexible characteristics of large language models (LLMs) with the accuracy of traditional programming. This type of AI acts autonomously to achieve a goal by using technologies like natural language processing (NLPs), machine learning, reinforcement learning and knowledge representation. It’s a proactive AI-powered [...] Agentic AI is the broader concept of solving issues with limited supervision, whereas an AI agent is a specific component within that system that is designed to handle tasks and processes with a degree of autonomy. This model is changing the way humans interact with AI. The agentic AI system is able to understand the goal or vision of the user and uses the information that is provided to solve a problem. [...] Problem-solving:Agentic AI uses a four-step approach for solving issues; perceive, reason, act, and learn. These four steps start by having AI agents gather and process data. The LLM then acts as an orchestrator that analyzes perceived data to understand the situation. And is then integrated with external tools that are continuously improving and learning through feedback.\", \"score\": 0.94632465, \"raw_content\": null}, {\"url\": \"https://research.aimultiple.com/agentic-ai/\", \"title\": \"40+ Agentic AI Use Cases with Real-life Examples in 2025\", \"content\": \"Agentic AI is the overarching framework that enables AI systems to solve problems with minimal supervision. Within this framework, AI agents are the individual components responsible for executing specific tasks autonomously.\\\\n\\\\nWhile agentic AI understands user goals and orchestrates the problem-solving process, AI agents perform the tasks.\\\\n\\\\n### Key features of agentic AI\", \"score\": 0.90593183, \"raw_content\": null}], \"response_time\": 2.88, \"request_id\": \"5384fa8b-a74c-4dfa-aa15-58534ed5a5c5\"}', name='tavily_search', tool_call_id='8fra71ckd')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply,\"wikipedia\":wiki_tool,\"tavily_search\":tavily_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "300b378b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The product of 5 and 15 is 75. Agentic AI refers to artificial intelligence systems designed to operate autonomously, making decisions and performing tasks with minimal human intervention. These systems utilize advanced techniques like natural language processing and machine learning to adaptively achieve their goals, distinguishing them from traditional AI by their ability to handle complex tasks independently.', additional_kwargs={'reasoning_content': 'Okay, so I need to figure out what the user is asking for. They provided a prompt that says \"What is 5 * 15 and also What is Agentic AI?\". Hmm, so they\\'re asking two separate questions here. The first one is a simple math problem, and the second is about a type of AI.\\n\\nFirst, let\\'s tackle the math question: 5 multiplied by 15. That seems straightforward. I remember that 5 times 10 is 50, and 5 times 5 is 25, so adding those together gives 75. So, 5 * 15 equals 75. That should be easy enough.\\n\\nNow, the second part is about Agentic AI. I\\'m not entirely sure what that is, so I\\'ll need to look it up. From the search results provided, it seems like Agentic AI refers to artificial intelligence systems that can operate autonomously, making decisions and performing tasks without much human intervention. It uses various AI techniques like natural language processing and machine learning to achieve its goals. It\\'s different from traditional AI because it\\'s more adaptive and can handle complex tasks on its own.\\n\\nSo, putting it all together, I need to provide the answer to both questions: the product of 5 and 15, and an explanation of what Agentic AI is based on the information I found.\\n'}, response_metadata={'token_usage': {'completion_tokens': 350, 'prompt_tokens': 3204, 'total_tokens': 3554, 'completion_time': 1.260859536, 'prompt_time': 0.298323026, 'queue_time': 0.045912475, 'total_time': 1.5591825620000002}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6f8b7e18-9378-450b-b6f9-ea98a23e6d68-0', usage_metadata={'input_tokens': 3204, 'output_tokens': 350, 'total_tokens': 3554})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 5 multiplied by 15 is 75.\n",
      "\n",
      "Agentic AI refers to a class of artificial intelligence systems designed to operate autonomously with minimal human intervention. These systems utilize advanced AI techniques such as natural language processing and machine learning to make decisions, perform tasks, and adapt to various situations without constant supervision. Unlike traditional AI models that follow predefined rules, Agentic AI can learn, reason, and execute complex workflows, making it highly effective in applications like customer support, cybersecurity, and business automation. It builds upon other AI technologies, focusing on autonomous decision-making and goal-oriented actions, thus advancing AI capabilities beyond content generation to task execution.\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tool.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a31dffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I will help you with each of your questions using the appropriate tools.\\n\\n1. For the multiplication:\\n', additional_kwargs={'reasoning_content': 'Alright, so the user asked three things: 25 multiplied by 5, what Agentic AI is, and a summary of recent climate change news. Let me break this down.\\n\\nFirst, the multiplication. That\\'s straightforward. 25 times 5 is 125. I can use the multiply tool for that.\\n\\nNext, explaining Agentic AI. Since it\\'s a specific term, I\\'ll need to look it up. I\\'ll use the Wikipedia tool with the query \"Agentic AI\" to get the details.\\n\\nLastly, the recent climate change news. For this, I should use the Tavily search tool. I\\'ll set the start date to the last 30 days to ensure the information is current. I\\'ll keep the search depth as advanced to get comprehensive results and make sure to exclude any irrelevant domains if needed.\\n\\nI need to structure the responses clearly, separating each part so the user can easily understand each answer.\\n', 'tool_calls': [{'id': 'y6v1bam3h', 'function': {'arguments': '{\"a\":25,\"b\":5}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 1807, 'total_tokens': 2129, 'completion_time': 1.168678338, 'prompt_time': 0.170948762, 'queue_time': 0.044573279, 'total_time': 1.3396271}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--537c1d3e-8929-4118-851c-ef92b781034f-0', tool_calls=[{'name': 'multiply', 'args': {'a': 25, 'b': 5}, 'id': 'y6v1bam3h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1807, 'output_tokens': 322, 'total_tokens': 2129})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is 25 * 5 and also What is Agentic AI and summarize the recent climate change news?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tool.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e385cf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 25 * 5 and also What is Agentic AI and summarize the recent climate change news?', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='10', name='multiply', tool_call_id='fewhat3by')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply, \"wikipedia\": wiki_tool, \"tavily_search\": tavily_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25864798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are the requested results:\\n\\n1. **Multiplication Result**:  \\n   25 multiplied by 5 equals **125**.\\n\\n2. **Agentic AI**:  \\n   Agentic AI is a type of artificial intelligence designed to perform tasks that typically require human intelligence. It is focused on enabling machines to learn, reason, and interact with their environment in ways that simulate human-like intelligence.\\n\\n3. **Recent Climate Change News**:  \\n   According to recent reports, global efforts to reduce carbon emissions are intensifying. The latest climate change summit emphasized the importance of international cooperation to meet Paris Agreement targets. Additionally, advancements in renewable energy technologies are showing promise in reducing greenhouse gas emissions.', additional_kwargs={'reasoning_content': 'Alright, so I need to figure out how to respond to the user\\'s query. They asked three things: what is 25 multiplied by 5, what Agentic AI is, and a summary of recent climate change news. \\n\\nFirst, for the multiplication part, that\\'s straightforward. I\\'ll call the multiply tool with a=25 and b=5. That should give me 125.\\n\\nNext, explaining Agentic AI. Since the user didn\\'t specify a date range, I should use the most recent information. I\\'ll use the Tavily search tool with the query \"Agentic AI\" and set the topic to general. I\\'ll make sure to exclude any specific domains unless the user mentioned any, which they didn\\'t. I\\'ll also set include_images to false since it\\'s an informational query.\\n\\nLastly, summarizing recent climate change news. For this, I\\'ll use Tavily again but this time set the topic to news and use the query \"recent climate change news\". I\\'ll set the time_range to \"month\" to get the latest updates from the past 30 days. This way, the user gets up-to-date and relevant information on climate change.\\n\\nI should present each part clearly, separating the multiplication result, the explanation of Agentic AI, and the climate change summary. Keeping each section concise and easy to read will help the user understand the information quickly.\\n'}, response_metadata={'token_usage': {'completion_tokens': 422, 'prompt_tokens': 1829, 'total_tokens': 2251, 'completion_time': 1.637032329, 'prompt_time': 0.173656452, 'queue_time': 0.045805358, 'total_time': 1.810688781}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3e17d636-85d7-472d-a3e1-ecd435259b15-0', usage_metadata={'input_tokens': 1829, 'output_tokens': 422, 'total_tokens': 2251})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dec7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba29c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51628a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainBasics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
